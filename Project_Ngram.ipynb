{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Language Classification Problem","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":"For this problem first lets check data.","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport csv","metadata":{"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"english_data = pd.read_csv(\"english.csv\")\nturkish_data = pd.read_csv(\"turkish.csv\")","metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"english_data.head()","metadata":{"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"     Word\n0     aah\n1   aahed\n2  aahing\n3    aahs\n4     aal","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>aah</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>aahed</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>aahing</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>aahs</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>aal</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"turkish_data.head()","metadata":{"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"     Word\n0   etmek\n1   olmak\n2     otu\n3      su\n4  bilimi","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>etmek</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>olmak</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>otu</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>su</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>bilimi</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"When I looked the head of data I detect some of abbreviations for english dataset.After checking full of data I detect many more abbreviations. In English dataset I saw suffix like 'ly','s',And as for turkish data We can't see verb suffix as \"yor\" \"dı\" \"tı\" but we can see \"mak\" for verbs. After these observations I think first we do preprocessing for this data like stemming and removing punctuation(there is not much of them but they are exist) one of the way to approuch this kind of data is character N-gram. But before we generating model lets do some preprocess.","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","source":"english_data['Language'] = 0\nturkish_data['Language'] = 1","metadata":{"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"I labeled data for future uses. In example below we can see punctuation in turkish data lets remove them. And if the word consists only of punctuations we remove that row.","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","source":"turkish_data[1155:1158]","metadata":{"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"        Word  Language\n1155     ...         1\n1156  gömlek         1\n1157   sebze         1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Language</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1155</th>\n      <td>...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1156</th>\n      <td>gömlek</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1157</th>\n      <td>sebze</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import string,nltk\ndef remove_punct(text):\n    text_without_punct = \"\".join([char for char in text if char not in string.punctuation])\n    return text_without_punct\nenglish_data['Word']=english_data['Word'].apply(lambda x: remove_punct(x))\nturkish_data['Word']=turkish_data['Word'].apply(lambda x: remove_punct(x))","metadata":{"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"turkish_data[1155:1158]","metadata":{"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"        Word  Language\n1155                 1\n1156  gömlek         1\n1157   sebze         1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Language</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1155</th>\n      <td></td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1156</th>\n      <td>gömlek</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1157</th>\n      <td>sebze</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"As we can see row 1155 was \"...\" and became \"\" Lets remove this kind of rows.","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","source":"turkish_data.replace(\"\", float(\"NaN\"), inplace=True)\nturkish_data.dropna(inplace=True)\nturkish_data[1155:1158]","metadata":{"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"        Word  Language\n1156  gömlek         1\n1157   sebze         1\n1158   karga         1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Language</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1156</th>\n      <td>gömlek</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1157</th>\n      <td>sebze</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1158</th>\n      <td>karga</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"ps=nltk.PorterStemmer()\ndef stemming(text):\n    stemmed_text = [ps.stem(word) for word in text]\n    return stemmed_text\nenglish_data['stemmed_words']= english_data['Word'].apply(lambda x: stemming([x]))","metadata":{"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"english_data[71:75]","metadata":{"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"            Word  Language stemmed_words\n71    abalienate         0     [abalien]\n72   abalienated         0     [abalien]\n73  abalienating         0     [abalien]\n74  abalienation         0     [abalien]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Language</th>\n      <th>stemmed_words</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>71</th>\n      <td>abalienate</td>\n      <td>0</td>\n      <td>[abalien]</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>abalienated</td>\n      <td>0</td>\n      <td>[abalien]</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>abalienating</td>\n      <td>0</td>\n      <td>[abalien]</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>abalienation</td>\n      <td>0</td>\n      <td>[abalien]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"As we can see stemming can effect actual context(abalienation's meaning is not close to abalien) so I will try both approuch(with stemming and without stemming).First try without stemming.(I didnt try both approuch because first one gave good results I should try it anyway but because of short amount of time I have, I didn't try it.)","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","source":"all_data = turkish_data.append(english_data, ignore_index=True)\nall_data.head()","metadata":{"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"     Word  Language stemmed_words\n0   etmek         1           NaN\n1   olmak         1           NaN\n2     otu         1           NaN\n3      su         1           NaN\n4  bilimi         1           NaN","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Language</th>\n      <th>stemmed_words</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>etmek</td>\n      <td>1</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>olmak</td>\n      <td>1</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>otu</td>\n      <td>1</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>su</td>\n      <td>1</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>bilimi</td>\n      <td>1</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"all_data.drop(['stemmed_words'], axis=1)","metadata":{"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                Word  Language\n0              etmek         1\n1              olmak         1\n2                otu         1\n3                 su         1\n4             bilimi         1\n...              ...       ...\n422079  zwinglianism         0\n422080  zwinglianist         0\n422081       zwitter         0\n422082    zwitterion         0\n422083  zwitterionic         0\n\n[422084 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Language</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>etmek</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>olmak</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>otu</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>su</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>bilimi</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>422079</th>\n      <td>zwinglianism</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>422080</th>\n      <td>zwinglianist</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>422081</th>\n      <td>zwitter</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>422082</th>\n      <td>zwitterion</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>422083</th>\n      <td>zwitterionic</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>422084 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(all_data['Word'],all_data['Language'], test_size=0.2, random_state=25)","metadata":{"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"Combining data, splitting it to train and test. And checking shapes to make sure everything is okay.","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","source":"print(X_train.shape)\nprint(X_test.shape)","metadata":{"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"(337667,)\n(84417,)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Lineer Regresion Model Building","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":"For model building I choice simplest approuch to begin with and before we create model we should vectorize our words with character n-gram algorithm.","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(ngram_range=(3, 3),analyzer='char')\nvectorizer.fit(X_train)","metadata":{"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"CountVectorizer(analyzer='char', ngram_range=(3, 3))"},"metadata":{}}]},{"cell_type":"code","source":"print(vectorizer.transform(['bağırsağı']))","metadata":{"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"  (0, 706)\t2\n  (0, 760)\t1\n  (0, 7945)\t1\n  (0, 8187)\t1\n  (0, 12026)\t1\n  (0, 12201)\t1\n","output_type":"stream"}]},{"cell_type":"code","source":"print(vectorizer.get_feature_names()[8187])","metadata":{"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"sağ\n","output_type":"stream"}]},{"cell_type":"markdown","source":"I checked if it works with simple exapmle at above","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","source":"X_train = vectorizer.transform(X_train)\nX_test  = vectorizer.transform(X_test)","metadata":{"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(max_iter=10000)\nclassifier.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"LogisticRegression(max_iter=10000)"},"metadata":{}}]},{"cell_type":"code","source":"score = classifier.score(X_test, y_test)\nprint(\"Accuracy:\", score)","metadata":{"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Accuracy: 0.9741876636222562\n","output_type":"stream"}]},{"cell_type":"code","source":"X_turkish_train,X_turkish_test,y_turkish_train,y_turkish_test = train_test_split(turkish_data['Word'],turkish_data['Language'], test_size=0.2, random_state=26)","metadata":{"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"X_turkish_test=vectorizer.transform(X_turkish_test)\nscore = classifier.score(X_turkish_test, y_turkish_test)\nprint(\"Accuracy:\", score)","metadata":{"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Accuracy: 0.8600470588235294\n","output_type":"stream"}]},{"cell_type":"code","source":"test_words = ['hovarda','haydaaa','hello','harika','helyum','helium']\nfor word in test_words:\n    if(classifier.predict(vectorizer.transform([word]))<1):\n        print(word + \": English\")\n    else:\n        print(word + \": Turkish\")","metadata":{"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"hovarda: Turkish\nhaydaaa: Turkish\nhello: English\nharika: Turkish\nhelyum: Turkish\nhelium: English\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Comments","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":"As we can see accuracy 0.974 but because of inbalance between datasets( turkish dataset have 53k english dataset have 368k data) for turkish words accuracy is close to 0.86 we can do some processing to increase this accuracy like : increasing weight of turkish words or maybe resample with different ratio or adding some rules. Ofcourse we can create neural network with multiple layer like RNN or Lstm to making it understand weights itself and getting better result with turkish words. I used 3-gram because of RAM limitage on Jupyter Lab(2Gb) but maybe 1-gram 2-gram or combination of 3 of them can give better results we can try it in a better enviroment(More RAM) just chaging by ngram_range.","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":"/////","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":"From here on out I saved my model and vectorizer to load them later and use them in API","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":"## Saving Model","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","source":"import pickle\nmodel_path = \"models/model.pickle\"\nvectorizer_path = \"models/vectorizer.pickle\"\npickle.dump(classifier, open(model_path, 'wb'))\npickle.dump(vectorizer, open(vectorizer_path, \"wb\"))","metadata":{"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"## Loading Model","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":"Loading must work before everything else(above) because we already have vectorizer and model at above so if we try to load them again with pickle it will give us error(To don't get this error restart kernel then come this section and run it)","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","source":"import pickle\nmodel_path = \"models/model.pickle\"\nvectorizer_path = \"models/vectorizer.pickle\"\n\nvectorizer = pickle.load(open(vectorizer_path,'rb'))\nclassifier = pickle.load(open(model_path,'rb'))\ntest_words = ['caz','blu','hell','karizmatik','ceku','müsamaha']\nfor word in test_words:\n    if(classifier.predict(vectorizer.transform([word]))<1):\n        print(\"English\")\n    else:\n        print(\"Turkish\")","metadata":{"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Turkish\nEnglish\nEnglish\nTurkish\nEnglish\nTurkish\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Creating API","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":"In here I created a basic API which get parameter Word(example: Word:'hello') that can be called by http://127.0.0.1:5000/predict?Word=example I don't know how to create working example in Jupyter Notebook for API but following code works fine in other enviroments. I didn't create API to get multiple prediction by sending multiple word because in assigment it say \"Word\" only so I though I should create very simple API. ","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","source":"import requests\nimport json\nfrom flask import Flask,request, jsonify\n\napp = Flask(__name__)\nclassifier_labels=[\"English\",\"Turkish\"]\n@app.route('/predict')\ndef predict():\n    req_word = request.args.get('Word')\n    label_index = classifier.predict(vectorizer.transform([req_word]))\n    label = classifier_labels[label_index[0]]\n    return jsonify(status='complete', label=label)\n\nif __name__ == '__main__':\n    app.run(debug=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}