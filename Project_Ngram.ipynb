{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Language Classification Problem","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":"For this problem first lets check data.","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport csv","metadata":{"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"english_data = pd.read_csv(\"english.csv\")\nturkish_data = pd.read_csv(\"turkish.csv\")","metadata":{"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"english_data.head()","metadata":{"trusted":true},"execution_count":60,"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"     Word\n0     aah\n1   aahed\n2  aahing\n3    aahs\n4     aal","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>aah</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>aahed</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>aahing</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>aahs</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>aal</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"turkish_data.head()","metadata":{"trusted":true},"execution_count":61,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"     Word\n0   etmek\n1   olmak\n2     otu\n3      su\n4  bilimi","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>etmek</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>olmak</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>otu</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>su</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>bilimi</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"When I looked the head of data I detect some of abbreviations for english dataset.After checking full of data I detect many more abbreviations. In English dataset I saw suffix like 'ly','s',And as for turkish data We can't see verb suffix as \"yor\" \"dı\" \"tı\" but we can see \"mak\" for verbs. After these observations I think first we do preprocessing for this data like stemming and removing punctions(there is not much of them but they are exist) one of the way to approuch this kind of data is character N-gram. But before we generating model lets split data for testing.","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","source":"english_data['Language'] = 0\nturkish_data['Language'] = 1","metadata":{"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"turkish_data[1155:1158]","metadata":{"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"        Word  Language\n1155     ...         1\n1156  gömlek         1\n1157   sebze         1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Language</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1155</th>\n      <td>...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1156</th>\n      <td>gömlek</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1157</th>\n      <td>sebze</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import string,nltk\ndef remove_punct(text):\n    text_without_punct = \"\".join([char for char in text if char not in string.punctuation])\n    return text_without_punct\nenglish_data['Word']=english_data['Word'].apply(lambda x: remove_punct(x))\nturkish_data['Word']=turkish_data['Word'].apply(lambda x: remove_punct(x))","metadata":{"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"turkish_data[1155:1158]","metadata":{"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"        Word  Language\n1155                 1\n1156  gömlek         1\n1157   sebze         1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Language</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1155</th>\n      <td></td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1156</th>\n      <td>gömlek</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1157</th>\n      <td>sebze</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"turkish_data.replace(\"\", float(\"NaN\"), inplace=True)\nturkish_data.dropna(inplace=True)\nturkish_data[1155:1158]","metadata":{"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"        Word  Language\n1156  gömlek         1\n1157   sebze         1\n1158   karga         1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Language</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1156</th>\n      <td>gömlek</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1157</th>\n      <td>sebze</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1158</th>\n      <td>karga</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"ps=nltk.PorterStemmer()\ndef stemming(text):\n    stemmed_text = [ps.stem(word) for word in text]\n    return stemmed_text\nenglish_data['stemmed_words']= english_data['Word'].apply(lambda x: stemming([x]))","metadata":{"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"english_data[71:75]","metadata":{"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"            Word  Language stemmed_words\n71    abalienate         0     [abalien]\n72   abalienated         0     [abalien]\n73  abalienating         0     [abalien]\n74  abalienation         0     [abalien]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Language</th>\n      <th>stemmed_words</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>71</th>\n      <td>abalienate</td>\n      <td>0</td>\n      <td>[abalien]</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>abalienated</td>\n      <td>0</td>\n      <td>[abalien]</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>abalienating</td>\n      <td>0</td>\n      <td>[abalien]</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>abalienation</td>\n      <td>0</td>\n      <td>[abalien]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"As we can see stemming can effect actual context so I will try both aprouch(with stemming and without stemming).First try without stemming.","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","source":"all_data = turkish_data.append(english_data, ignore_index=True)\nall_data.head()","metadata":{"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"     Word  Language stemmed_words\n0   etmek         1           NaN\n1   olmak         1           NaN\n2     otu         1           NaN\n3      su         1           NaN\n4  bilimi         1           NaN","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Language</th>\n      <th>stemmed_words</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>etmek</td>\n      <td>1</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>olmak</td>\n      <td>1</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>otu</td>\n      <td>1</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>su</td>\n      <td>1</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>bilimi</td>\n      <td>1</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"all_data.drop(['stemmed_words'], axis=1)","metadata":{"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"                Word  Language\n0              etmek         1\n1              olmak         1\n2                otu         1\n3                 su         1\n4             bilimi         1\n...              ...       ...\n422079  zwinglianism         0\n422080  zwinglianist         0\n422081       zwitter         0\n422082    zwitterion         0\n422083  zwitterionic         0\n\n[422084 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Language</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>etmek</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>olmak</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>otu</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>su</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>bilimi</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>422079</th>\n      <td>zwinglianism</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>422080</th>\n      <td>zwinglianist</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>422081</th>\n      <td>zwitter</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>422082</th>\n      <td>zwitterion</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>422083</th>\n      <td>zwitterionic</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>422084 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(all_data['Word'],all_data['Language'], test_size=0.2, random_state=25)","metadata":{"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)\nprint(X_test.shape)","metadata":{"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"(337667,)\n(84417,)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Lineer Regresion Model Building","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(ngram_range=(3, 3),analyzer='char')\nvectorizer.fit(X_train)","metadata":{"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"CountVectorizer(analyzer='char', ngram_range=(3, 3))"},"metadata":{}}]},{"cell_type":"code","source":"print(vectorizer.get_feature_names()[8187])\nprint(vectorizer.transform(['bağırsağı']))","metadata":{"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"sağ\n  (0, 706)\t2\n  (0, 760)\t1\n  (0, 7945)\t1\n  (0, 8187)\t1\n  (0, 12026)\t1\n  (0, 12201)\t1\n","output_type":"stream"}]},{"cell_type":"code","source":"X_train = vectorizer.transform(X_train)\nX_test  = vectorizer.transform(X_test)","metadata":{"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"<337667x12575 sparse matrix of type '<class 'numpy.int64'>'\n\twith 2442348 stored elements in Compressed Sparse Row format>"},"metadata":{}}]},{"cell_type":"markdown","source":"When I looked the head of data I detect some of abbreviations for english dataset.After checking full of data I detect many more abbreviations. In English dataset I saw suffix like 'ly','s',And as for turkish data We can't see verb suffix as \"yor\" \"dı\" \"tı\" but we can see \"mak\" for verbs. After these observations I think first we do preprocessing for this data like stemming and removing punctions(there is not much of them but they are exist) one of the way to approuch this kind of data is character N-gram. But before we generating model lets split data for testing.","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(max_iter=10000)\nclassifier.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"LogisticRegression(max_iter=10000)"},"metadata":{}}]},{"cell_type":"code","source":"score = classifier.score(X_test, y_test)\nprint(\"Accuracy:\", score)","metadata":{"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Accuracy: 0.9741876636222562\n","output_type":"stream"}]},{"cell_type":"code","source":"X_turkish_train,X_turkish_test,y_turkish_train,y_turkish_test = train_test_split(turkish_data['Word'],turkish_data['Language'], test_size=0.2, random_state=26)","metadata":{"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"X_turkish_test=vectorizer.transform(X_turkish_test)\nscore = classifier.score(X_turkish_test, y_turkish_test)\nprint(\"Accuracy:\", score)","metadata":{"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Accuracy: 0.8600470588235294\n","output_type":"stream"}]},{"cell_type":"code","source":"test_words = ['hovarda','haydaaa','hello','harika','helyum','helium']\nfor word in test_words:\n    if(classifier.predict(vectorizer.transform([word]))<1):\n        print(\"English\")\n    else:\n        print(\"Turkish\")","metadata":{"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"Turkish\nTurkish\nEnglish\nTurkish\nTurkish\nEnglish\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Comments","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":"As we can see accuracy 0.974 but because of inbalance between datasets( turkish dataset have 53k english dataset have 368k data) for turkish words accuracy is close to 0.86 we can do some processing to increase this accuracy like : increasing weight of turkish words or maybe resample with different ratio or adding some rules. Ofcourse we can create neural network with multiple layer like RNN or Lstm to making it understand weights itself and getting better result with turkish words. I used 3-gram because of RAM limitage on Jupyter Lab(2Gb) but maybe 1-gram 2-gram or combination of 3 of them can give better results we can try it in a better enviroment(More RAM) just chaging by ngram_range.","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":"/////","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":"From here on out I saved my model and vectorizer to load them later and use them in API","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","source":"import pickle\nmodel_path = \"models/model.pickle\"\nvectorizer_path = \"models/vectorizer.pickle\"\npickle.dump(classifier, open(model_path, 'wb'))\npickle.dump(vectorizer, open(vectorizer_path, \"wb\"))","metadata":{"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-328d5eb94464>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"models/model.pickle\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvectorizer_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"models/vectorizer.pickle\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'classifier' is not defined"],"ename":"NameError","evalue":"name 'classifier' is not defined","output_type":"error"}]},{"cell_type":"code","source":"import pickle\nmodel_path = \"models/model.pickle\"\nvectorizer_path = \"models/vectorizer.pickle\"\n\nvectorizer = pickle.load(open(vectorizer_path,'rb'))\nclassifier = pickle.load(open(model_path,'rb'))\ntest_words = ['caz','blu','hell','karizmatik','ceku','müsamaha']\nfor word in test_words:\n    if(classifier.predict(vectorizer.transform([word]))<1):\n        print(\"English\")\n    else:\n        print(\"Turkish\")","metadata":{"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Turkish\nEnglish\nEnglish\nTurkish\nEnglish\nTurkish\n","output_type":"stream"}]},{"cell_type":"code","source":"import requests\nimport json\ndef jprint(obj):\n    # create a formatted string of the Python JSON object\n    text = json.dumps(obj, sort_keys=True, indent=4)\n    print(text)\nresponse = requests.get(\"https://api.thedogapi.com/v1/breeds\")","metadata":{"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import flask\nfrom flask import request, jsonify\n\napp = flask.Flask(__name__)\napp.config[\"DEBUG\"] = False\n\n# Create some test data for our catalog in the form of a list of dictionaries.\nbooks = [\n    {'id': 0,\n     'title': 'A Fire Upon the Deep',\n     'author': 'Vernor Vinge',\n     'first_sentence': 'The coldsleep itself was dreamless.',\n     'year_published': '1992'},\n    {'id': 1,\n     'title': 'The Ones Who Walk Away From Omelas',\n     'author': 'Ursula K. Le Guin',\n     'first_sentence': 'With a clamor of bells that set the swallows soaring, the Festival of Summer came to the city Omelas, bright-towered by the sea.',\n     'published': '1973'},\n    {'id': 2,\n     'title': 'Dhalgren',\n     'author': 'Samuel R. Delany',\n     'first_sentence': 'to wound the autumnal city.',\n     'published': '1975'}\n]\n\n\n@app.route('/', methods=['GET'])\ndef home():\n    return '''<h1>dev.to/koraybarkin Flask ile Web API geliştirme</h1><p>Tebrikler ilk Web API'ınızı başarıyla geliştirdiniz!</p>'''\n\n\n# A route to return all of the available entries in our catalog.\n@app.route('/api/v1/resources/books/all', methods=['GET'])\ndef api_all():\n    return jsonify(books)\n\napp.run()","metadata":{"trusted":true},"execution_count":13,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-e7bad6271a7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mflask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mflask\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjsonify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mapp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"DEBUG\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'flask'"],"ename":"ModuleNotFoundError","evalue":"No module named 'flask'","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}